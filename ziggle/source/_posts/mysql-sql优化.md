---
title: mysql-sql优化
date: 2020-10-09 22:36:52
tags: 
- mysql
---
#### 问题:
对于小的偏移量，直接使用limit来查询没有什么问题，但随着数据量的增大，越往后分页，limit语句的偏移量就会越大，速度也会明显变慢。
优化思想:
避免数据量大时扫描过多的记录
#### 解决:
子查询的分页方式或者JOIN分页方式。
JOIN分页和子查询分页的效率基本在一个等级上，消耗的时间也基本一致。
下面举个例子。一般MySQL的主键是自增的数字类型，这种情况下可以使用下面的方式进行优化。
下面以真实的生产环境的80万条数据的一张表为例，比较一下优化前后的查询耗时:

```sql
-- 传统limit，文件扫描
[SQL]SELECT * FROM tableName ORDER BY id LIMIT 500000,2;
受影响的行: 0
时间: 5.371s

-- 子查询方式，索引扫描
[SQL]
SELECT * FROM tableName
WHERE id >= (SELECT id FROM tableName ORDER BY id LIMIT 500000 , 1)
LIMIT 2;
受影响的行: 0
时间: 0.274s

-- JOIN分页方式
[SQL]
SELECT *
FROM tableName AS t1
JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 1) AS t2
WHERE t1.id <= t2.id ORDER BY t1.id desc LIMIT 2;
受影响的行: 0
时间: 0.278s
```

复制代码可以看到经过优化性能提高了将近20倍。

#### 优化原理:
子查询是在索引上完成的，而普通的查询时在数据文件上完成的，通常来说，索引文件要比数据文件小得多，所以操作起来也会更有效率。因为要取出所有字段内容，第一种需要跨越大量数据块并取出，而第二种基本通过直接根据索引字段定位后，才取出相应内容，效率自然大大提升。
因此，对limit的优化，不是直接使用limit，而是首先获取到offset的id，然后直接使用limit size来获取数据。
在实际项目使用，可以利用类似策略模式的方式去处理分页，例如，每页100条数据，判断如果是100页以内，就使用最基本的分页方式，大于100，则使用子查询的分页方式。


#### 对uuid处理
因为插入值会随机写到索引的不同位置,使得insert更慢,这会导致页分裂,磁盘随机访问,以及对聚集索引产生碎片,
逻辑上相邻的行被分布在磁盘和内存的不同位置
随机值导致缓存对所有类型的查询语句效果很差,使得缓存依赖以工作的访问局部性原理失效,

如果存储uuid应该移除 '-' 符号,或者更好的做法是,用UNHEX()函数转换uuid为16字节的数字,并且存储在一个binary(16)列中,

#### 物化视图
物化视图是预先计算并存储在磁盘上的表,通过各种策略刷新和更新,mysql不支持(flexviews)

#### alter table 
mysql 执行大部分 `alter table`操作 使用新的结构创建按一个空表, 从旧表中查出所有的数据插入新表,然后删除旧表. 当内存不足而且表又很大时,而且有很多索引的情况下尤其如此, 

